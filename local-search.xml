<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>pd.DataFrame and pd.read_csv</title>
    <link href="/2024/01/03/pandas/"/>
    <url>/2024/01/03/pandas/</url>
    
    <content type="html"><![CDATA[<p>This blog is about the key use and attributes of a famous data structure from pandas, pd.DataFrame, including fetch a row, column and element, as well as the transformation of List and DataFrame. </p><h2 id="一-读取数据pd-read-csv-filepath"><a href="#一-读取数据pd-read-csv-filepath" class="headerlink" title="一. 读取数据pd.read_csv(filepath)"></a>一. 读取数据<code>pd.read_csv(filepath)</code></h2><h3 id="1-Args-说明"><a href="#1-Args-说明" class="headerlink" title="1. Args 说明"></a>1. Args 说明</h3><ol><li><p><code>filePath = &#39;path/to/your/example.csv&#39;</code>  </p></li><li><p><strong>default</strong>将第一行读取为<strong>列头</strong>，如果第一列为数据，则指定 <code>header=None</code>,此时会添加默认列头(0,1,2,3,…)</p></li></ol><h3 id="2-读取excel文件"><a href="#2-读取excel文件" class="headerlink" title="2. 读取excel文件"></a>2. 读取excel文件</h3><ol><li>使用如下代码将excel转化成csv</li></ol><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs nsis">file_path_excel = <span class="hljs-string">&#x27;path/to/your/example.xlsx&#x27;</span>  <span class="hljs-comment"># Specify the path to your Excel file</span><br>csv_output_path = <span class="hljs-string">&#x27;path/to/your/example.csv&#x27;</span>    <span class="hljs-comment"># Specify the desired path for the CSV output</span><br><br><span class="hljs-comment"># Convert Excel to CSV using Excel&#x27;s Save As feature or with pandas</span><br><span class="hljs-comment"># For example, you can use pandas to_csv method</span><br>df_excel = pd.read_excel(file_path_excel, sheet_name=<span class="hljs-string">&#x27;Sheet1&#x27;</span>)<br>df_excel.to_csv(csv_output_path, index=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># Now use pd.read_csv to read the CSV file</span><br>df_csv = pd.read_csv(csv_output_path)<br></code></pre></td></tr></table></figure><ol start="2"><li>或者直接更改后缀xlsx-&gt;csv(可能出现格式错误问题)</li></ol><h3 id="3-分隔符"><a href="#3-分隔符" class="headerlink" title="3. 分隔符"></a>3. 分隔符</h3><p>详情请见<a href="https://blog.csdn.net/weixin_50294842/article/details/122930229?spm=1001.2014.3001.5501">使用pd.read_csv的分隔符问题–清三皮</a></p><h2 id="二-pd-DataFrame类型"><a href="#二-pd-DataFrame类型" class="headerlink" title="二. pd.DataFrame类型"></a>二. pd.DataFrame类型</h2><h3 id="1-整体信息"><a href="#1-整体信息" class="headerlink" title="1. 整体信息"></a>1. 整体信息</h3><ol><li>获取大小<code>df.shape</code><br>Output (numbers of rows, numbers of columns)</li><li>列名称<code>df.columns</code></li><li>行名称<code>df.index</code></li><li>表头<code>df.head()</code></li><li>统计一列的值信息<code>df[&#39;Normal/Attack&#39;].value_counts()</code></li></ol><h3 id="2-Access"><a href="#2-Access" class="headerlink" title="2. Access"></a>2. Access</h3><pre><code class="hljs">如果需要通过名称访问，则使用loc如果根据索引访问，则使用iloc  【表头算是第0行】</code></pre><ol><li><strong>根据列名</strong>访问一列<br><code>df[&#39;col1_name&#39;]</code><br><code>df[[&#39;col1_name&#39;,&#39;col2_name&#39;]]</code>  </li><li><strong>根据索引</strong>访问一列<br><code>df.iloc[:,1]</code><strong>访问第二列</strong></li><li>访问一行<br><code>df.iloc[1]</code><strong>访问第二行</strong>效果同<code>df.iloc[1,:]</code><br>访问几行<br><code>df.iloc[0:2]</code><strong>iloc函数左闭右开</strong></li><li>按条件访问<br><code>df.loc[data.B&gt;6]</code></li></ol><h3 id="3-用List新建一列"><a href="#3-用List新建一列" class="headerlink" title="3. 用List新建一列"></a>3. 用List新建一列</h3><p><code>df[&#39;weight&#39;]=weight_list</code><br><code>weight_list = list(df[&#39;weight&#39;])</code></p><h3 id="4-新建DataFrame"><a href="#4-新建DataFrame" class="headerlink" title="4. 新建DataFrame"></a>4. 新建DataFrame</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">f1</span>=[<span class="hljs-number">99</span>.<span class="hljs-number">06</span>,<span class="hljs-number">99</span>.<span class="hljs-number">15</span>,<span class="hljs-number">97</span>.<span class="hljs-number">77</span>,<span class="hljs-number">98</span>.<span class="hljs-number">40</span>,<span class="hljs-number">99</span>.<span class="hljs-number">25</span>]<br><span class="hljs-attribute">precision</span>=[<span class="hljs-number">98</span>.<span class="hljs-number">97</span>,<span class="hljs-number">99</span>.<span class="hljs-number">18</span>,<span class="hljs-number">96</span>.<span class="hljs-number">67</span>,<span class="hljs-number">99</span>.<span class="hljs-number">80</span>,<span class="hljs-number">99</span>.<span class="hljs-number">52</span>]<br><span class="hljs-attribute">accuracy</span>=[<span class="hljs-number">99</span>.<span class="hljs-number">06</span>,<span class="hljs-number">99</span>.<span class="hljs-number">15</span>,<span class="hljs-number">97</span>.<span class="hljs-number">73</span>,<span class="hljs-number">98</span>.<span class="hljs-number">41</span>,<span class="hljs-number">99</span>.<span class="hljs-number">27</span>]<br><span class="hljs-attribute">recall</span>=[<span class="hljs-number">99</span>.<span class="hljs-number">16</span>,<span class="hljs-number">99</span>.<span class="hljs-number">12</span>,<span class="hljs-number">99</span>.<span class="hljs-number">98</span>,<span class="hljs-number">97</span>.<span class="hljs-number">03</span>,<span class="hljs-number">98</span>.<span class="hljs-number">99</span>]<br>  <br><span class="hljs-attribute">import</span> pandas as pd<br><span class="hljs-attribute">result</span>=pd.DataFrame(&#123;&#x27;f1&#x27;:f1,&#x27;recall&#x27;:recall,&#x27;precision&#x27;:precision,&#x27;accuracy&#x27;:accuracy&#125;).applymap(<span class="hljs-string">&quot;&#123;:.04f&#125;&quot;</span>.format)<br><span class="hljs-attribute">result</span>.loc[len(result)]=[sum(recall)/<span class="hljs-number">5</span>,sum(f1)/<span class="hljs-number">5</span>,sum(accuracy)/<span class="hljs-number">5</span>,sum(precision)/<span class="hljs-number">5</span>]<br><span class="hljs-attribute">result</span>.rename(index=&#123;<span class="hljs-number">5</span>:&#x27;average&#x27;&#125;,inplace=True)<br><span class="hljs-attribute">result</span><br><br></code></pre></td></tr></table></figure><p><strong>output</strong><br>！<a href="../image/post/DataFrame.png" title="output">dataframe</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>the basic use of Pytorch</title>
    <link href="/2024/01/02/pytorch/"/>
    <url>/2024/01/02/pytorch/</url>
    
    <content type="html"><![CDATA[<p>This blog is written to introduce the basic use of pytorch, including the construction of custom dataset and neural network, as well as the function of training and validation.</p><h2 id="一-训练前"><a href="#一-训练前" class="headerlink" title="一. 训练前"></a>一. 训练前</h2><h3 id="1-优化函数、损失函数选择"><a href="#1-优化函数、损失函数选择" class="headerlink" title="1. 优化函数、损失函数选择"></a>1. 优化函数、损失函数选择</h3><pre><code class="hljs">`import torch`    `from torch.utils.data import Dataset`</code></pre><ol><li><p><code>torch.nn</code>中有诸多损失函数，常用的如<code>loss_f = torch.nn.CrossEntropyLoss()</code>  </p></li><li><p><code>torch.optim</code>中为优化函数，常用的如<code>opt = torch.optim.Adam(module.parameters(),lr=learning_rate)</code>  </p><p> 其中第一个参数为模型的所有参数，第二个参数为超参数学习率  </p></li><li><p>模型的参数可由如下代码获得,并且在训练之前设置参数为可优化<br> <code>for param in model.parameters():</code><br> <code>   param.requires_grad = True</code></p></li><li><p>学习率在训练中动态调整，<code>torch.optim.lr_scheduler</code>中存放了调整函数，常用的如<code>scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(eta_min = 1e-6,optimizer = opt,T_0=5)</code></p></li></ol><h3 id="2-GPU"><a href="#2-GPU" class="headerlink" title="2. GPU"></a>2. GPU</h3><ol><li><p>选择可用的GPU <code>device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; </code>  </p></li><li><p>将模型、损失函数移动到device上<br> <code>model = model.to(device=device)</code><br> <code>loss_f = loss_f.to(device=device)</code>  </p></li><li><p>在训练过程中将数据移动到cuda上，示例代码如下  </p><p> <code>for batch, datas in enumerate(train_loader):</code><br> <code>   x, y = datas</code><br> <code>   if device == &#39;cuda&#39;: x, y = x.cuda(),y.cuda()</code></p></li></ol><h2 id="二-数据集"><a href="#二-数据集" class="headerlink" title="二. 数据集"></a>二. 数据集</h2><h3 id="1-继承torch自带的数据集类"><a href="#1-继承torch自带的数据集类" class="headerlink" title="1. 继承torch自带的数据集类"></a>1. 继承torch自带的数据集类</h3><pre><code class="hljs">`from torch.utils.data import Dataset`  </code></pre><h4 id="1-1-类声明"><a href="#1-1-类声明" class="headerlink" title="1.1 类声明"></a>1.1 类声明</h4><p>   <code>class yourDatasetName(Dataset):</code><br>   重写<br>   <code>def __init__(self,data,label) -&gt; None:</code><br>   <code>def __len__(self):</code><br>   <code>def __getitem__(self,idx)</code></p><h4 id="1-2-def-init-parameter…"><a href="#1-2-def-init-parameter…" class="headerlink" title="1.2 def init(parameter…)"></a>1.2 def <strong>init</strong>(parameter…)</h4><pre><code class="hljs">该函数用提供的（参数）data和标签构建数据集，转化data和label为`torch.tensor结构`，同时以类成员的方式保存转化后的结果。</code></pre><h5 id="三种常见init方式"><a href="#三种常见init方式" class="headerlink" title="三种常见init方式"></a>三种常见init方式</h5><ol><li><p>传入data,label方式构建  </p><p><strong>Args说明</strong><br><code>data (list or numpy array)</code><br><code>labels (list or numpy array)</code><br> <strong>转换</strong><br> <code>self.data = torch.tensor(data)</code><br> <code>self.labels = torch.tensor(labels)</code>  </p></li><li><p>传入pd.DataFrame方式构建</p><p><strong>Args说明</strong><br><code>dataframe : pd.DataFrame</code><br><code>datafram[&#39;labels&#39;]存储标签信息</code><br><strong>转换</strong><br><code>self.data = torch.tensor(dataframe.values)</code><br><code>self.labels = torch.tensor(dataframe[&#39;labels&#39;].values)</code>  </p></li><li><p>传入数据集路径构建</p></li></ol><h4 id="1-3-def-len-self"><a href="#1-3-def-len-self" class="headerlink" title="1.3 def len(self)"></a>1.3 def <strong>len</strong>(self)</h4><pre><code class="hljs">该函数返回所有数据集中样本的总数，该函数被Dataloader调用从而决定每一个epoch的批量大小</code></pre><p><code>return len(self.data)</code></p><h4 id="1-4-def-getitem-self-index"><a href="#1-4-def-getitem-self-index" class="headerlink" title="1.4 def getitem(self,index)"></a>1.4 def <strong>getitem</strong>(self,index)</h4><pre><code class="hljs">该函数根据给定的index返回下标所指的data和对应label，被Dataloader调用来取特定的下标元素，返回的data和label需要tuple形式  </code></pre><p><code>return self.data[index],self.labels[index]</code></p><h3 id="2-DataLoader"><a href="#2-DataLoader" class="headerlink" title="2. DataLoader"></a>2. DataLoader</h3><pre><code class="hljs">from torch.utils.data import DataLoader  </code></pre><h4 id="2-1-实例化数据集类"><a href="#2-1-实例化数据集类" class="headerlink" title="2.1 实例化数据集类"></a>2.1 实例化数据集类</h4><p>   <code>my_dataset = yourDatasetName(args....)</code>  </p><h4 id="2-2-参数"><a href="#2-2-参数" class="headerlink" title="2.2 参数"></a>2.2 参数</h4><p>   <code>batch_size = 32</code>&#x2F;&#x2F;越大越好，大了可能gpu受不住<br>   <code>shuffle = True</code> &#x2F;&#x2F;在每个epoch开始前打乱一边数据<br>   <code>num_workers = 2</code>  </p><h4 id="2-3-创建DataLoader"><a href="#2-3-创建DataLoader" class="headerlink" title="2.3 创建DataLoader"></a>2.3 创建DataLoader</h4><p>   <code>my_dataloader = DataLoader(my_dataset)</code> </p><h4 id="2-4-使用"><a href="#2-4-使用" class="headerlink" title="2.4 使用"></a>2.4 使用</h4><p>   <code>for batch_data, batch_labels in my_dataloader:</code>  </p><h2 id="三-模型选择与构建"><a href="#三-模型选择与构建" class="headerlink" title="三. 模型选择与构建"></a>三. 模型选择与构建</h2><pre><code class="hljs">import torchimport torch.nn as nnimport torch.nn.functional as F</code></pre><h3 id="1-自己构建子类"><a href="#1-自己构建子类" class="headerlink" title="1. 自己构建子类"></a>1. 自己构建子类</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">MyNeuralNetwork</span>, <span class="hljs-variable language_">self</span>).__init__()<br><br>        <span class="hljs-comment"># Define your layers</span><br>        <span class="hljs-variable language_">self</span>.fc1 = nn.<span class="hljs-title class_">Linear</span>(input_size, hidden_size)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.<span class="hljs-title class_">Linear</span>(hidden_size, output_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, x</span>):<br>        <span class="hljs-comment"># Define the forward pass</span><br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = <span class="hljs-variable language_">self</span>.fc2(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><ol><li>init方法初始化神经网络的所有层  </li><li>forward方法定义前向传播</li></ol><h3 id="2-残差块"><a href="#2-残差块" class="headerlink" title="2. 残差块"></a>2. 残差块</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Residual</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>, <span class="hljs-title">input_channels</span>, <span class="hljs-title">output_channels</span>, <span class="hljs-title">conv1_stride</span>, <span class="hljs-title">conv2_stride</span>, <span class="hljs-title">use_1x1conv</span>=<span class="hljs-type">False</span>) -&gt; <span class="hljs-type">None</span>:</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.conv1 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">input_channels</span>, <span class="hljs-title">output_channels</span>,</span><br><span class="hljs-class">                               <span class="hljs-title">kernel_size</span>=(3, 1), padding=(1, 0),</span><br><span class="hljs-class">                               stride=conv1_stride)</span><br><span class="hljs-class">        self.conv2 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">output_channels</span>, <span class="hljs-title">output_channels</span>,</span><br><span class="hljs-class">                               <span class="hljs-title">kernel_size</span>=(3, 1), padding=(1, 0),</span><br><span class="hljs-class">                               stride=conv2_stride)</span><br><span class="hljs-class">        self.bn1 = nn.<span class="hljs-type">BatchNorm2d</span>(<span class="hljs-title">output_channels</span>)</span><br><span class="hljs-class">        self.bn2 = nn.<span class="hljs-type">BatchNorm2d</span>(<span class="hljs-title">output_channels</span>)</span><br><span class="hljs-class">        if use_1x1conv:</span><br><span class="hljs-class">            self.conv1x1 = nn.<span class="hljs-type">Conv2d</span>(</span><br><span class="hljs-class">                <span class="hljs-title">input_channels</span>, <span class="hljs-title">output_channels</span>, <span class="hljs-title">kernel_size</span>=1, <span class="hljs-title">stride</span>=<span class="hljs-title">conv1_stride</span>)</span><br><span class="hljs-class">        else:</span><br><span class="hljs-class">            self.conv1x1 = <span class="hljs-type">None</span></span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-type">X</span>):</span><br><span class="hljs-class">        <span class="hljs-type">Y</span> = <span class="hljs-type">F</span>.relu(<span class="hljs-title">self</span>.<span class="hljs-title">bn1</span>(<span class="hljs-title">self</span>.<span class="hljs-title">conv1</span>(<span class="hljs-type">X</span>)))</span><br><span class="hljs-class">        <span class="hljs-type">Y</span> = (<span class="hljs-title">self</span>.<span class="hljs-title">bn2</span>(<span class="hljs-title">self</span>.<span class="hljs-title">conv2</span>(<span class="hljs-type">Y</span>)))</span><br><span class="hljs-class">        if self.conv1x1:</span><br><span class="hljs-class">            <span class="hljs-type">X</span> = self.conv1x1(<span class="hljs-type">X</span>)</span><br><span class="hljs-class">        <span class="hljs-type">Y</span> += <span class="hljs-type">X</span></span><br><span class="hljs-class">        return <span class="hljs-type">F</span>.relu(<span class="hljs-type">Y</span>)</span><br><span class="hljs-class"></span><br></code></pre></td></tr></table></figure><h3 id="3-初始卷积层"><a href="#3-初始卷积层" class="headerlink" title="3. 初始卷积层"></a>3. 初始卷积层</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> stem(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, input_channels, output_channels</span>) -&gt; <span class="hljs-title class_">None</span>:<br>        <span class="hljs-variable language_">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.conv = nn.<span class="hljs-title class_">Conv2d</span>(input_channels, output_channels,<br>                              kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), stride=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.bn = nn.<span class="hljs-title class_">BatchNorm2d</span>(output_channels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, X</span>):<br>        <span class="hljs-keyword">return</span> F.relu(<span class="hljs-variable language_">self</span>.bn(<span class="hljs-variable language_">self</span>.conv(X)))<br></code></pre></td></tr></table></figure><h3 id="4-叠buff"><a href="#4-叠buff" class="headerlink" title="4. 叠buff"></a>4. 叠buff</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> model(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>) -&gt; <span class="hljs-type">None</span>:</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.embed=nn.<span class="hljs-type">Embedding</span>(256,119)</span><br><span class="hljs-class">        self.stem=stem(1,16)</span><br><span class="hljs-class">        self.ress=nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-type">Residual</span>(16, 32, 1, 1, <span class="hljs-title">use_1x1conv</span>=<span class="hljs-type">True</span>),</span><br><span class="hljs-class">            <span class="hljs-type">Residual</span>(32, 128, 2, 1, <span class="hljs-title">use_1x1conv</span>=<span class="hljs-type">True</span>),</span><br><span class="hljs-class">            <span class="hljs-type">Residual</span>(128, 256, 2, 1, <span class="hljs-title">use_1x1conv</span>=<span class="hljs-type">True</span>),</span><br><span class="hljs-class">            <span class="hljs-type">Residual</span>(256, 512, 2, 1, <span class="hljs-title">use_1x1conv</span>=<span class="hljs-type">True</span>),</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">        self.avgpool=nn.<span class="hljs-type">AdaptiveAvgPool2d</span>(1)</span><br><span class="hljs-class">        self.linear=nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Flatten</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(512,26)</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-type">X</span>):</span><br><span class="hljs-class">        <span class="hljs-type">X</span>=self.embed(<span class="hljs-type">X</span>)</span><br><span class="hljs-class">        <span class="hljs-type">X</span>=self.stem(<span class="hljs-type">X</span>)</span><br><span class="hljs-class">        <span class="hljs-type">X</span>=self.ress(<span class="hljs-type">X</span>)</span><br><span class="hljs-class">        return self.linear(<span class="hljs-title">self</span>.<span class="hljs-title">avgpool</span>(<span class="hljs-type">X</span>))</span><br></code></pre></td></tr></table></figure><h2 id="四-训练"><a href="#四-训练" class="headerlink" title="四. 训练"></a>四. 训练</h2><figure class="highlight leaf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs leaf">def train(model, device, train_loader, optimizer, criterion,scheduler):<br>    time_start=time.time()<br>    predictions = []<br>    targets = []<br>    train_recall=[]<br>    train_acc=[]<br>    train_loss=[]<br>    model.train()<br>    for batch, datas in enumerate(train_loader):<br>        <span class="hljs-punctuation">#</span><span class="hljs-keyword">print</span><span class="hljs-params">(<span class="hljs-string">&quot;batch numbers:&#123;&#125;&quot;</span>.<span class="hljs-keyword">format</span><span class="hljs-params">(<span class="hljs-keyword">len</span><span class="hljs-params">(<span class="hljs-variable">train_loader</span>)</span>)</span>)</span><br>        x, y = datas<br>        if device == &#x27;cuda&#x27;:<br>            x, y = x.cuda(), y.cuda()<br><br>        <span class="hljs-punctuation">#</span> 输入一个批量的数据，x.shape为[batch_size,num_features]<br>        y_hat,featuremap = model(x.reshape(-1, 1, 119).long())<br>        <span class="hljs-punctuation">#</span> print(featuremap,featuremap.size())<br>        del featuremap<br>        <span class="hljs-punctuation">#</span> 输出为批量中每个元素的预测值，y_hat.shape为[batch_size,num_classes]<br>        _, pre_y = torch.max(y_hat, dim=1)<br>        loss = criterion(y_hat, y)<br>        acc = torch.sum(y == pre_y)<br>        <span class="hljs-punctuation">#</span> recall=recall_score(pre_y,y,average=&#x27;micro&#x27;)<br>        y,pre_y,acc=y.cpu(),pre_y.cpu(),acc.cpu()<br>        recall=recall_score(y,pre_y,average=metrix_mode,zero_division=1)<br>        <br>        train_recall.append(recall)<br>        train_acc.append(acc)<br>        loss.backward()<br>        optimizer.step()<br>        optimizer.zero_grad()<br>        train_loss.append(loss.cpu().data)<br>        predictions.extend(pre_y.cpu().numpy())<br>        targets.extend(y.cpu().numpy())<br>        scheduler.step()<br>        if (batch+1)%20==0:<br>            time_end=time.time()<br>            <span class="hljs-punctuation">#</span><span class="hljs-keyword">print</span><span class="hljs-params">(<span class="hljs-string">&quot;pre:&#123;&#125;,target:&#123;&#125;&quot;</span>.<span class="hljs-keyword">format</span><span class="hljs-params">(<span class="hljs-variable">predictions</span>,<span class="hljs-variable">targets</span>)</span>)</span><br>            <span class="hljs-punctuation">#</span><span class="hljs-keyword">print</span><span class="hljs-params">(<span class="hljs-string">&quot;pre:&quot;</span>,<span class="hljs-keyword">Counter</span><span class="hljs-params">(<span class="hljs-variable">predictions</span>)</span>)</span><br>            <span class="hljs-punctuation">#</span><span class="hljs-keyword">print</span><span class="hljs-params">(<span class="hljs-string">&quot;tar:&quot;</span>,<span class="hljs-keyword">Counter</span><span class="hljs-params">(<span class="hljs-variable">targets</span>)</span>)</span><br>            print(&quot;batch:&#123;&#125;/&#123;&#125;,loss:&#123;:.5f&#125;,train_recall:&#123;:.5f&#125;,train_acc:&#123;:.5f&#125;,time cost:&#123;:.2f&#125;&quot;.format(<br>                batch+1,<br>                len(train_loader),<br>                np.mean(train_loss)/batch_size,<br>                np.mean(train_recall),<br>                np.mean(train_acc)/batch_size,<br>                time_end-time_start))<br>            time_start=time.time()<br>    f1 = f1_score(targets, predictions,average=metrix_mode,zero_division=1)<br>    precision = precision_score(targets, predictions,average=metrix_mode,zero_division=1)<br>    accuracy = accuracy_score(targets, predictions)<br>    recall = recall_score(targets, predictions,average=metrix_mode,zero_division=1)<br><br>    return f1, precision, accuracy, recall<br></code></pre></td></tr></table></figure><h2 id="五-验证"><a href="#五-验证" class="headerlink" title="五. 验证"></a>五. 验证</h2><figure class="highlight leaf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs leaf">def evaluate(model, device, data_loader,criterion):<br>    model.eval()<br>    predictions = []<br>    targets = []<br>    valid_loss=[]<br>    with torch.no_grad():<br>        for batch,datas in enumerate(data_loader):<br>            data,target=datas<br>            data, target = data.to(device), target.to(device)<br>            output,featuremap = model(data.reshape(-1,1,119).long())<br>            del featuremap<br>            loss=criterion(output,target)<br>            valid_loss.append(loss.cpu().data)<br>            pred = output.argmax(dim=1, keepdim=True)<br>            <br>            predictions.extend(pred.cpu().numpy())<br>            targets.extend(target.cpu().numpy())<br>            if (batch+1)%5==0:<br>                print(&quot;batch&#123;&#125;,valid loss:&#123;:.5f&#125;,valid acc:&#123;:.4f&#125;&quot;.format(<br>                        batch+1,np.mean(valid_loss)/batch_size,accuracy_score(targets, predictions)))<br><br>    f1 = f1_score(targets, predictions,average=metrix_mode,zero_division=1)<br>    precision = precision_score(targets, predictions,average=metrix_mode,zero_division=1)<br>    accuracy = accuracy_score(targets, predictions)<br>    recall = recall_score(targets, predictions,average=metrix_mode,zero_division=1)<br>    <span class="hljs-punctuation">#</span><span class="hljs-keyword">print</span><span class="hljs-params">(<span class="hljs-keyword">Counter</span><span class="hljs-params">(<span class="hljs-variable">predictions</span>)</span>)</span><br>    <span class="hljs-punctuation">#</span><span class="hljs-keyword">print</span><span class="hljs-params">(<span class="hljs-keyword">Counter</span><span class="hljs-params">(<span class="hljs-variable">targets</span>)</span>)</span><br>    return f1, precision, accuracy, recall<br></code></pre></td></tr></table></figure><h2 id="六-训练-k-fold-crossValidation"><a href="#六-训练-k-fold-crossValidation" class="headerlink" title="六. 训练+k-fold-crossValidation"></a>六. 训练+k-fold-crossValidation</h2><p><code>skf=KFold(n_splits=k,shuffle=True,random_state=8)</code>  </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># start training</span><br>f1_scores=[]<br>precision_scores=[]<br>accuracy_scores=[]<br>recall_scores=[]<br><br><span class="hljs-keyword">for</span> fold, (train_indices, test_indices) <span class="hljs-keyword">in</span> enumerate(skf.split(train_set_full)):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Fold&#123;&#125;/&#123;&#125;&quot;</span>.format(fold+1, k))<br><br>    <span class="hljs-attribute">train_sampler</span>=torch.utils.data.WeightedRandomSampler(<br>        weights[train_indices],<br>        40000,<br>        <span class="hljs-literal">True</span><br>    )<br>    <span class="hljs-attribute">test_sampler</span>=torch.utils.data.WeightedRandomSampler(<br>        weights[test_indices],<br>        10000,<br>        <span class="hljs-literal">True</span><br>    )<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;init sampler success&quot;</span>)<br>    train_loader = torch.utils.data.DataLoader(<br>        torch.utils.data.Subset(train_set_full, train_indices),<br>        <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>,<br>        <span class="hljs-attribute">batch_size</span>=batch_size, <br>        )<br>    <span class="hljs-attribute">valid_loader</span>=torch.utils.data.DataLoader(<br>        torch.utils.data.Subset(train_set_full,test_indices),<br>        <span class="hljs-attribute">batch_size</span>=batch_size,<br>    )<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;init dataloader success&quot;</span>)<br>    module = model()<br>    module = module.<span class="hljs-keyword">to</span>(<span class="hljs-attribute">device</span>=device)<br>    loss_f = torch.nn.CrossEntropyLoss()<br>    loss_f = loss_f.<span class="hljs-keyword">to</span>(<span class="hljs-attribute">device</span>=device)<br>    opt = torch.optim.Adam(module.parameters(), <span class="hljs-attribute">lr</span>=learning_rate)<br>   <span class="hljs-built_in"> scheduler </span>= torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(<br>            <span class="hljs-attribute">eta_min</span>=1e-6,<br>            <span class="hljs-attribute">optimizer</span>=opt,<br>            <span class="hljs-attribute">T_0</span>=5)<br>    <br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epoche_num):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch:&#123;&#125;/&#123;&#125;&quot;</span>.format(epoch+1,epoche_num))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;----training----&quot;</span>)<br>        train(<span class="hljs-attribute">model</span>=module,<br>              <span class="hljs-attribute">device</span>=device,<br>              <span class="hljs-attribute">train_loader</span>=train_loader,<br>              <span class="hljs-attribute">optimizer</span>=opt,<br>              <span class="hljs-attribute">criterion</span>=loss_f,<br>              <span class="hljs-attribute">scheduler</span>=scheduler)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;train finished,begin valid&quot;</span>)<br>        evaluate(module, <br>                 device, <br>                 valid_loader,<br>                 loss_f)<br>        <br>    <span class="hljs-attribute">time_foldstart</span>=time.time()<br>    f1, precision, accuracy, recall = evaluate(module, device, valid_loader,loss_f)<br>    <span class="hljs-attribute">time_foldend</span>=time.time()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;time cost:&#123;:.2f&#125;&quot;</span>.format(time_foldend-time_foldstart))<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;F1-score: &#123;f1&#125;&quot;</span>)<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Precision: &#123;precision&#125;&quot;</span>)<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Accuracy: &#123;accuracy&#125;&quot;</span>)<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Recall: &#123;recall&#125;&quot;</span>)<br><br>    # Append the evaluation metrics <span class="hljs-keyword">to</span> the lists<br>    f1_scores.append(f1)<br>    precision_scores.append(precision)<br>    accuracy_scores.append(accuracy)<br>    recall_scores.append(recall)<br>torch.save(module,<span class="hljs-string">&#x27;module&#x27;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo+Fluid+Github</title>
    <link href="/2024/01/01/hexo+github%E4%BD%BF%E7%94%A8/"/>
    <url>/2024/01/01/hexo+github%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>Welcome to my first blog! </p><p>This blog is written to introduce and record the basic use of hexo, as well as posting blog to github.  </p><p>In order to deploy blog on github.io, Nodejs and Hexo are required to be installed in your computer.  </p><p>Additionally, this blog also introduce the SSH connecting from local computer to your github accounts. </p><h2 id="发博客"><a href="#发博客" class="headerlink" title="发博客"></a>发博客</h2><ol><li><p>每次在source写完博客后，在root目录下使用 <code>hexo g</code> 生成页面  </p></li><li><p>然后可以选择使用<code>hexo s</code>本地进行预览  </p></li><li><p>使用<code>hexo d</code>推送到github.io</p></li></ol><h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><ol><li><p>根据每个theme不同的需求进行安装</p></li><li><p>需要修改原_config.yaml文件中的配置  </p><blockquote><p>修改author等信息，对应到每篇文章末尾的信息戳   </p></blockquote></li><li><p>新建_config.themeName.yml,将目标theme的config内容复制进该文件中</p><ol><li>新建yml的方式【新建记事本、vscode打开、另存为.yml文件】</li></ol></li></ol><h3 id="以更换fluid主题为例"><a href="#以更换fluid主题为例" class="headerlink" title="以更换fluid主题为例"></a>以更换fluid主题为例</h3><h4 id="安装目标主题"><a href="#安装目标主题" class="headerlink" title="安装目标主题"></a>安装目标主题</h4><blockquote><p>npm install –save hexo-theme-fluid</p></blockquote><h4 id="修改hexo博客目录中的-config-yml"><a href="#修改hexo博客目录中的-config-yml" class="headerlink" title="修改hexo博客目录中的_config.yml"></a>修改hexo博客目录中的_config.yml</h4><blockquote><p>theme: fluid  &#x2F;&#x2F;原来为landscape  </p><p>language: zh-CN &#x2F;&#x2F; 原来为EN</p></blockquote><h4 id="其他使用未完待续"><a href="#其他使用未完待续" class="headerlink" title="其他使用未完待续"></a>其他使用未完待续</h4><p>详情请见<a href="https://hexo.fluid-dev.com/">fluid主题官网</a></p><h2 id="与github搭配使用"><a href="#与github搭配使用" class="headerlink" title="与github搭配使用"></a>与github搭配使用</h2><h3 id="建立仓库"><a href="#建立仓库" class="headerlink" title="建立仓库"></a>建立仓库</h3><pre><code class="hljs">1. 在github新建仓库，取名为lightningSr.github.io2. 将该仓库的setting中配置为SSH</code></pre><h3 id="配置SSH"><a href="#配置SSH" class="headerlink" title="配置SSH"></a>配置SSH</h3><h4 id="·本地SSH"><a href="#·本地SSH" class="headerlink" title="·本地SSH"></a>·本地SSH</h4><ol><li><p>配置git  </p><p> <code>git config --global user.name &quot;lightningSr&quot;</code>  </p><p> <code>git config --global user.email &quot;15110281946@163.com&quot;</code></p></li><li><p>生成SSH  </p><p> <code>ssh-keygen -t rsa -C &quot;15110281946@163.com&quot; </code>  </p><pre><code class="hljs"> ·生成密钥，成功标志是命令行见到以邮箱结尾的rsa密钥和签名信息   ·生成的密钥写入了.ssh\id_rsa.pub文件中</code></pre></li><li><p>复制SSH，准备配置github信息【windows】  </p><p> <code>type C:\Users\lightning\.ssh\id_rsa.pub | clip</code></p></li></ol><h4 id="·github配置SSH"><a href="#·github配置SSH" class="headerlink" title="·github配置SSH"></a>·github配置SSH</h4><pre><code class="hljs">在github-setting-SSH and GPG keys 中，选择new SSH key  将刚刚复制的密钥粘贴进去</code></pre><h4 id="·验证"><a href="#·验证" class="headerlink" title="·验证"></a>·验证</h4><pre><code class="hljs">终端中输入 `ssh -T git@github.com`</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Fluid</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
